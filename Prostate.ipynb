{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1DlwqsHFj3gZ1OgdlSxp20_-uwXd6MHDc",
      "authorship_tag": "ABX9TyNG12Mb7TckexkgoM2Qz6vY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pickles91/DCAC/blob/main/Prostate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. GPU acceleration is available?\n",
        "\n"
      ],
      "metadata": {
        "id": "0UQHKB4ClgmX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZQZRlvHX2WyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51bdb6b4-fafe-4398-e0bf-f4c37af1069b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "GPU acceleration is available.\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check whether GPU-accelerated computing is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"------------------------------\")\n",
        "    print(\"GPU acceleration is available.\")\n",
        "    print(\"------------------------------\")\n",
        "else:\n",
        "    print(\"------------------------------------------------------------------------------------\")\n",
        "    print(\"Error: GPU acceleration is not available. Please enable GPU in the Runtime settings.\")\n",
        "    print(\"------------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Installing nnU-Net"
      ],
      "metadata": {
        "id": "hZfvaPemllL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install nnunet\n",
        "!pip install nnunetv2"
      ],
      "metadata": {
        "id": "DE_e6WpPlwov",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33b1aa75-3400-4a69-afc3-c6d59bb96ab0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnunetv2\n",
            "  Downloading nnunetv2-2.4.1.tar.gz (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.1/184.1 kB\u001b[0m \u001b[31m971.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (2.2.1+cu121)\n",
            "Collecting acvl-utils<0.3,>=0.2 (from nnunetv2)\n",
            "  Downloading acvl_utils-0.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dynamic-network-architectures<0.4,>=0.3.1 (from nnunetv2)\n",
            "  Downloading dynamic_network_architectures-0.3.1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (4.66.2)\n",
            "Collecting dicom2nifti (from nnunetv2)\n",
            "  Downloading dicom2nifti-2.4.10-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (1.11.4)\n",
            "Collecting batchgenerators>=0.25 (from nnunetv2)\n",
            "  Downloading batchgenerators-0.25.tar.gz (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (1.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (0.19.3)\n",
            "Collecting SimpleITK>=2.2.1 (from nnunetv2)\n",
            "  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (2.0.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (0.20.3)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (2024.4.18)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (2.31.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (4.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (0.13.1)\n",
            "Collecting imagecodecs (from nnunetv2)\n",
            "  Downloading imagecodecs-2024.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.6/39.6 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yacs (from nnunetv2)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting connected-components-3d (from acvl-utils<0.3,>=0.2->nnunetv2)\n",
            "  Downloading connected_components_3d-3.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2) (9.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2) (0.18.3)\n",
            "Collecting unittest2 (from batchgenerators>=0.25->nnunetv2)\n",
            "  Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2) (3.4.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2) (3.3)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2) (2.31.6)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.1.2->nnunetv2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.1.2->nnunetv2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.1.2->nnunetv2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.1.2->nnunetv2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.1.2->nnunetv2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.1.2->nnunetv2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.1.2->nnunetv2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.1.2->nnunetv2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.1.2->nnunetv2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=2.1.2->nnunetv2)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.1.2->nnunetv2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.2->nnunetv2)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting pydicom>=2.2.0 (from dicom2nifti->nnunetv2)\n",
            "  Downloading pydicom-2.4.4-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-gdcm (from dicom2nifti->nnunetv2)\n",
            "  Downloading python_gdcm-3.0.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel->nnunetv2) (67.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nnunetv2) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nnunetv2) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nnunetv2) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs->nnunetv2) (6.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->nnunetv2) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.2->nnunetv2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.2->nnunetv2) (1.3.0)\n",
            "Collecting argparse (from unittest2->batchgenerators>=0.25->nnunetv2)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting traceback2 (from unittest2->batchgenerators>=0.25->nnunetv2)\n",
            "  Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting linecache2 (from traceback2->unittest2->batchgenerators>=0.25->nnunetv2)\n",
            "  Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: nnunetv2, acvl-utils, batchgenerators, dynamic-network-architectures\n",
            "  Building wheel for nnunetv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nnunetv2: filename=nnunetv2-2.4.1-py3-none-any.whl size=248797 sha256=3f8cdf64c61b27ca26dab51ddc66e271b2f581f116ffed9df36f00693758100c\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/35/c0/c13533b9a4962005e4e18379831a436a0e657765911c770491\n",
            "  Building wheel for acvl-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for acvl-utils: filename=acvl_utils-0.2-py3-none-any.whl size=22439 sha256=6b3fd57fcb7bddb3f824d6f016243569689a49bf2efc1b4a32c36c95ab1fcce3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/f0/84/52e8897591e66339bd2796681b9540b6c5e453c1461fa92a9e\n",
            "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for batchgenerators: filename=batchgenerators-0.25-py3-none-any.whl size=89007 sha256=aa4336798895527821de144660e59afab113a53ef234518659af2cf619ca1da2\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/b0/1b/40912fb58eb167b86cbc444ddb2e6ba382b248215295f932e2\n",
            "  Building wheel for dynamic-network-architectures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.3.1-py3-none-any.whl size=30049 sha256=f05170d82192736c416404583690344eadcd9d8cce655cfbf889f87c25624e18\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/1b/13/a6419c8dbf998b9343710355ec3edc5c8e24d9b7b22eec95fb\n",
            "Successfully built nnunetv2 acvl-utils batchgenerators dynamic-network-architectures\n",
            "Installing collected packages: SimpleITK, linecache2, argparse, yacs, traceback2, python-gdcm, pydicom, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, imagecodecs, connected-components-3d, unittest2, nvidia-cusparse-cu12, nvidia-cudnn-cu12, dicom2nifti, nvidia-cusolver-cu12, batchgenerators, dynamic-network-architectures, acvl-utils, nnunetv2\n",
            "Successfully installed SimpleITK-2.3.1 acvl-utils-0.2 argparse-1.4.0 batchgenerators-0.25 connected-components-3d-3.14.1 dicom2nifti-2.4.10 dynamic-network-architectures-0.3.1 imagecodecs-2024.1.1 linecache2-1.0.0 nnunetv2-2.4.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pydicom-2.4.4 python-gdcm-3.0.23.1 traceback2-1.4.0 unittest2-1.1.0 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              },
              "id": "0ae6f189aa5b424fbddcf3a4f6a0f74f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: you do not have to restart the Runtime even when the following error appears, this is simply due to a reinstallation of a package - so no worries.\n",
        "\n",
        "\n",
        "```\n",
        "WARNING: The following packages were previously imported in this runtime:\n",
        "\n",
        "[argparse]\n",
        "\n",
        "You must restart the runtime in order to use newly installed versions.\n",
        "```"
      ],
      "metadata": {
        "id": "9hFA2RQnmEhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if nnunet can be imported\n",
        "import nnunetv2"
      ],
      "metadata": {
        "id": "9ITZwCCVmLrM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Import Packages for Google Colab"
      ],
      "metadata": {
        "id": "4xFLyqKTl1kO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ebu5jAXJY11h"
      },
      "outputs": [],
      "source": [
        "# Import basic packages\n",
        "import os\n",
        "import shutil\n",
        "from collections import OrderedDict\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import nibabel as nib\n",
        "\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Connect Google Colab with GoogleDrive"
      ],
      "metadata": {
        "id": "JZwpkdxvmXRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "jmqMdPd4hjd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8486eba-f914-4f8c-ba91-6289ba30b56e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Creating directories for nnUNet data and models\n"
      ],
      "metadata": {
        "id": "5IOCRKf5msej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating directories for nnUNet data and models\n",
        "!mkdir -p \"/content/drive/My Drive/Colab Notebooks/nnUNet_raw\"\n",
        "!mkdir -p \"/content/drive/My Drive/Colab Notebooks/nnUNet_preprocessed\"\n",
        "!mkdir -p \"/content/drive/MyDrive/Colab Notebooks/nnUNet_trained_models\"\n",
        "!mkdir -p \"/content/drive/My Drive/Colab Notebooks/nnUNet_results\"\n",
        "\n",
        "\n",
        "# Changing directory to nnUNet_raw_data to download and extract data\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/nnUNet_raw\"\n",
        "\n",
        "# Downloading and extracting the data for Task05_Prostate\n",
        "!gdown https://drive.google.com/uc?id=1Ff7c21UksxyT4JfETjaarmuKEjdqe1-a\n",
        "!tar -xvf Task05_Prostate.tar\n",
        "!rm Task05_Prostate.tar\n",
        "\n",
        "# Changing directory back to the parent directory\n",
        "%cd .."
      ],
      "metadata": {
        "id": "UZhLMiTvhm2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acf78de5-f91e-4811-b21b-63b6064fdc63"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/nnUNet_raw\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Ff7c21UksxyT4JfETjaarmuKEjdqe1-a\n",
            "From (redirected): https://drive.google.com/uc?id=1Ff7c21UksxyT4JfETjaarmuKEjdqe1-a&confirm=t&uuid=9780ac8f-1756-417d-92ea-c063d38816c2\n",
            "To: /content/drive/MyDrive/Colab Notebooks/nnUNet_raw/Task05_Prostate.tar\n",
            "100% 240M/240M [00:01<00:00, 209MB/s]\n",
            "Task05_Prostate/._dataset.json\n",
            "Task05_Prostate/dataset.json\n",
            "Task05_Prostate/._imagesTr\n",
            "Task05_Prostate/imagesTr/\n",
            "Task05_Prostate/imagesTr/prostate_16.nii.gz\n",
            "Task05_Prostate/imagesTr/._prostate_04.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_04.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_32.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_20.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_43.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_18.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_06.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_14.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_41.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_34.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_38.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_10.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_02.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_24.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_47.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_28.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_00.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_42.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_21.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_17.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_40.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_31.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_07.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_35.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_44.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_39.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_01.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_13.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_46.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_25.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_29.nii.gz\n",
            "Task05_Prostate/imagesTr/prostate_37.nii.gz\n",
            "Task05_Prostate/._imagesTs\n",
            "Task05_Prostate/imagesTs/\n",
            "Task05_Prostate/imagesTs/prostate_08.nii.gz\n",
            "Task05_Prostate/imagesTs/._prostate_22.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_22.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_30.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_45.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_26.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_36.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_12.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_33.nii.gz\n",
            "Task05_Prostate/imagesTs/._prostate_09.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_09.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_05.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_23.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_19.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_15.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_11.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_03.nii.gz\n",
            "Task05_Prostate/imagesTs/prostate_27.nii.gz\n",
            "Task05_Prostate/._labelsTr\n",
            "Task05_Prostate/labelsTr/\n",
            "Task05_Prostate/labelsTr/prostate_16.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_04.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_32.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_20.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_43.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_18.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_06.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_14.nii.gz\n",
            "Task05_Prostate/labelsTr/._prostate_41.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_41.nii.gz\n",
            "Task05_Prostate/labelsTr/._prostate_34.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_34.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_38.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_10.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_02.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_24.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_47.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_28.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_00.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_42.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_21.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_17.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_40.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_31.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_07.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_35.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_44.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_39.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_01.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_13.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_46.nii.gz\n",
            "Task05_Prostate/labelsTr/._prostate_25.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_25.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_29.nii.gz\n",
            "Task05_Prostate/labelsTr/prostate_37.nii.gz\n",
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Maybe move path of preprocessed data directly on content - this may be signifcantely faster!\n",
        "drive_dir = \"/content/drive/My Drive\"\n",
        "mount_dir = os.path.join(drive_dir, \"Colab Notebooks\")\n",
        "base_dir = os.getcwd()\n",
        "\n",
        "print(\"Current Working Directory {}\".format(os.getcwd()))\n",
        "print(\" \")\n",
        "path_dict = {\n",
        "    \"nnUNet_raw\" : os.path.join(mount_dir, \"nnUNet_raw\"),\n",
        "    \"nnUNet_preprocessed\" : os.path.join(mount_dir, \"nnUNet_preprocessed\"), # 1 experiment: 1 epoch took 112s\n",
        "    \"nnUNet_results\" : os.path.join(mount_dir, \"nnUNet_results\"),\n",
        "}\n",
        "\n",
        "# Write paths to environment variables\n",
        "for env_var, path in path_dict.items():\n",
        "  os.environ[env_var] = path\n",
        "\n",
        "def make_if_dont_exist(folder_path,overwrite=False):\n",
        "    \"\"\"\n",
        "    creates a folder if it does not exists\n",
        "    input:\n",
        "    folder_path : relative path of the folder which needs to be created\n",
        "    over_write :(default: False) if True overwrite the existing folder\n",
        "    \"\"\"\n",
        "    if os.path.exists(folder_path):\n",
        "\n",
        "        if not overwrite:\n",
        "            print(f\"{folder_path} exists.\")\n",
        "        else:\n",
        "            print(f\"{folder_path} overwritten\")\n",
        "            shutil.rmtree(folder_path)\n",
        "            os.makedirs(folder_path)\n",
        "\n",
        "    else:\n",
        "      os.makedirs(folder_path)\n",
        "      print(f\"{folder_path} created!\")\n",
        "\n",
        "# Check whether all environment variables are set correct!\n",
        "for env_var, path in path_dict.items():\n",
        "  if os.getenv(env_var) != path:\n",
        "    print(\"Error:\")\n",
        "    print(\"Environment Variable {} is not set correctly!\".format(env_var))\n",
        "    print(\"Should be {}\".format(path))\n",
        "    print(\"Variable is {}\".format(os.getenv(env_var)))\n",
        "  make_if_dont_exist(path, overwrite=False)\n",
        "\n",
        "print(\" \")\n",
        "print(\"If No Error Occured Continue Forward. =)\")"
      ],
      "metadata": {
        "id": "tTBnHJCKaU-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d3d03f9-a36c-4b86-e386-25639ca1b160"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory /content/drive/MyDrive/Colab Notebooks\n",
            " \n",
            "/content/drive/My Drive/Colab Notebooks/nnUNet_raw exists.\n",
            "/content/drive/My Drive/Colab Notebooks/nnUNet_preprocessed exists.\n",
            "/content/drive/My Drive/Colab Notebooks/nnUNet_results exists.\n",
            " \n",
            "If No Error Occured Continue Forward. =)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install virtualenv package, create a virtual environment named 'myenv',\n",
        "# activate the virtual environment, and install a package named 'package_name'.\n",
        "!pip install virtualenv\n",
        "!virtualenv myenv\n",
        "!source myenv/bin/activate; pip install package_name"
      ],
      "metadata": {
        "id": "OOnjQXcgKlWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4bed7b2-d6a2-4855-b54f-af3bb1e238d0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.10/dist-packages (20.25.3)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (0.3.8)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.13.4)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (4.2.0)\n",
            "created virtual environment CPython3.10.12.final.0-64 in 11534ms\n",
            "  creator CPython3Posix(dest=/content/drive/My Drive/Colab Notebooks/myenv, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: package_name==0.1, pip==24.0, setuptools==69.5.1, wheel==0.43.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n",
            "Requirement already satisfied: package_name in ./myenv/lib/python3.10/site-packages (0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx3FsUbncL-f"
      },
      "source": [
        "### Dataset Conversion\n",
        "The Decathlon datasets are 4D nifti files, for nnU-Net they have to be converted to 3D nifti files.\n",
        "\n",
        "For more information about dataset conversion see: [nnU-Net Dataset Formatting Instructions](https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/dataset_format.md)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nnUNetv2_convert_MSD_dataset -i \"/content/drive/MyDrive/Colab Notebooks/nnUNet_raw/Task05_Prostate\""
      ],
      "metadata": {
        "id": "JoNMOytjiEVj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xh8Rjy5cL-g"
      },
      "source": [
        "## Extracting Rule Based Parameters\n",
        "This will preprocess the dataset to allow fast training and saves it into the \"nnUNet_preprocessed\" folder.\n",
        "Further rule based parameters will be extracted in the planning step."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the Execution of nnU-Net for Dataset 5\n",
        "!nnUNetv2_plan_and_preprocess -d 5"
      ],
      "metadata": {
        "id": "Pq6_ohXPd2Kq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11bfb3d4-b7bd-49d6-e1fe-d168c0ac83d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fingerprint extraction...\n",
            "Dataset005_Prostate\n",
            "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
            "100% 32/32 [00:32<00:00,  1.03s/it]\n",
            "Experiment planning...\n",
            "\n",
            "############################\n",
            "INFO: You are using the old nnU-Net default planner. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
            "############################\n",
            "\n",
            "Dropping 3d_lowres config because the image size difference to 3d_fullres is too small. 3d_fullres: [ 20. 320. 319.], 3d_lowres: [20, 320, 319]\n",
            "2D U-Net configuration:\n",
            "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 32, 'patch_size': (320, 320), 'median_image_size_in_voxels': array([320., 319.]), 'spacing': array([0.625, 0.625]), 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': (32, 64, 128, 256, 512, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
            "\n",
            "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
            "3D fullres U-Net configuration:\n",
            "{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (20, 320, 256), 'median_image_size_in_voxels': array([ 20., 320., 319.]), 'spacing': array([3.5999999, 0.625    , 0.625    ]), 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': (32, 64, 128, 256, 320, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((1, 3, 3), (1, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (1, 2, 2), (1, 2, 2), (2, 2, 2), (2, 2, 2), (1, 2, 2), (1, 2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': False}\n",
            "\n",
            "Plans were saved to /content/drive/My Drive/Colab Notebooks/nnUNet_preprocessed/Dataset005_Prostate/nnUNetPlans.json\n",
            "Preprocessing...\n",
            "Preprocessing dataset Dataset005_Prostate\n",
            "Configuration: 2d...\n",
            "100% 32/32 [00:53<00:00,  1.66s/it]\n",
            "Configuration: 3d_fullres...\n",
            "100% 32/32 [00:45<00:00,  1.44s/it]\n",
            "Configuration: 3d_lowres...\n",
            "INFO: Configuration 3d_lowres not found in plans file nnUNetPlans.json of dataset Dataset005_Prostate. Skipping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training nnU-Net\n",
        "here we will train a 3D nnU-Net on Full Resolution for 2 epochs.\n",
        "\n",
        "To run a normal training use: ```nnUNetTrainer``` instead of ```nnUNetTrainer_1epoch```"
      ],
      "metadata": {
        "id": "TvaCVx8Witz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the 3d nnUnet on the Full Resolution with Dataset 5 and Cross Validation Split 0\n",
        "!nnUNetv2_train 5 3d_fullres 0 -tr nnUNetTrainer_1epoch\n",
        "\n",
        "# Training for 2 iterations will take about 11 minutes"
      ],
      "metadata": {
        "id": "K20FuAtsiWwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035a5e57-ece4-4dea-faeb-969a74a83ab3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "############################\n",
            "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
            "############################\n",
            "\n",
            "Using device: cuda:0\n",
            "\n",
            "#######################################################################\n",
            "Please cite the following paper when using nnU-Net:\n",
            "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
            "#######################################################################\n",
            "\n",
            "2024-04-23 02:35:16.160782: do_dummy_2d_data_aug: True\n",
            "2024-04-23 02:35:16.166652: Creating new 5-fold cross-validation split...\n",
            "2024-04-23 02:35:16.175877: Desired fold for training: 0\n",
            "2024-04-23 02:35:16.179792: This split has 25 training and 7 validation cases.\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "using pin_memory on device 0\n",
            "using pin_memory on device 0\n",
            "2024-04-23 02:35:30.148587: Using torch.compile...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
            "\n",
            "This is the configuration used by this training:\n",
            "Configuration name: 3d_fullres\n",
            " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [20, 320, 256], 'median_image_size_in_voxels': [20.0, 320.0, 319.0], 'spacing': [3.5999999046325684, 0.625, 0.625], 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 320, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} \n",
            "\n",
            "These are the global plan.json settings:\n",
            " {'dataset_name': 'Dataset005_Prostate', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [3.5999999046325684, 0.625, 0.625], 'original_median_shape_after_transp': [20, 320, 320], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1326.0, 'mean': 350.18780517578125, 'median': 327.0, 'min': 0.0, 'percentile_00_5': 83.0, 'percentile_99_5': 822.0, 'std': 139.9563751220703}, '1': {'max': 3698.0, 'mean': 1351.1083984375, 'median': 1364.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 2563.0, 'std': 376.4326477050781}}} \n",
            "\n",
            "2024-04-23 02:35:32.837652: unpacking dataset...\n",
            "2024-04-23 02:36:00.788878: unpacking done...\n",
            "2024-04-23 02:36:00.820571: Unable to plot network architecture: nnUNet_compile is enabled!\n",
            "2024-04-23 02:36:00.863353: \n",
            "2024-04-23 02:36:00.877644: Epoch 0\n",
            "2024-04-23 02:36:00.880780: Current learning rate: 0.01\n",
            "2024-04-23 02:46:34.095130: train_loss -0.1908\n",
            "2024-04-23 02:46:34.118477: val_loss -0.4624\n",
            "2024-04-23 02:46:34.122259: Pseudo dice [0.2723, 0.7955]\n",
            "2024-04-23 02:46:34.125406: Epoch time: 633.24 s\n",
            "2024-04-23 02:46:34.131974: Yayy! New best EMA pseudo Dice: 0.5339\n",
            "2024-04-23 02:46:40.999875: Training done.\n",
            "2024-04-23 02:46:41.230893: Using splits from existing split file: /content/drive/My Drive/Colab Notebooks/nnUNet_preprocessed/Dataset005_Prostate/splits_final.json\n",
            "2024-04-23 02:46:41.262261: The split file contains 5 splits.\n",
            "2024-04-23 02:46:41.298626: Desired fold for training: 0\n",
            "2024-04-23 02:46:41.347001: This split has 25 training and 7 validation cases.\n",
            "2024-04-23 02:46:41.503911: predicting prostate_00\n",
            "2024-04-23 02:46:41.597948: prostate_00, shape torch.Size([2, 17, 307, 307]), rank 0\n",
            "2024-04-23 02:47:50.317328: predicting prostate_04\n",
            "2024-04-23 02:47:50.366762: prostate_04, shape torch.Size([2, 17, 306, 307]), rank 0\n",
            "2024-04-23 02:47:51.901517: predicting prostate_14\n",
            "2024-04-23 02:47:51.977397: prostate_14, shape torch.Size([2, 20, 320, 319]), rank 0\n",
            "2024-04-23 02:47:53.677061: predicting prostate_20\n",
            "2024-04-23 02:47:53.849597: prostate_20, shape torch.Size([2, 20, 320, 320]), rank 0\n",
            "2024-04-23 02:47:55.761942: predicting prostate_25\n",
            "2024-04-23 02:47:55.834590: prostate_25, shape torch.Size([2, 19, 320, 319]), rank 0\n",
            "2024-04-23 02:47:57.650305: predicting prostate_31\n",
            "2024-04-23 02:47:57.693411: prostate_31, shape torch.Size([2, 20, 320, 319]), rank 0\n",
            "2024-04-23 02:47:59.259131: predicting prostate_42\n",
            "2024-04-23 02:47:59.284387: prostate_42, shape torch.Size([2, 22, 320, 319]), rank 0\n",
            "2024-04-23 02:48:49.253253: Validation complete\n",
            "2024-04-23 02:48:49.257559: Mean Validation Dice:  0.5275879747632796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with Pre-Trained Models on Decathlon Prostate Dataset (WIP)\n",
        "Here we will use a pretrained model on the Prostate Dataset and visualize the results"
      ],
      "metadata": {
        "id": "EiBMMTkMjFLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading Pretrained Model\n",
        "First Download the pretrained models and validate that everything works correctly"
      ],
      "metadata": {
        "id": "QzwGEAQJjJg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Pretrained Model for the Prostate Dataset (Decathlon Dataset)\n",
        "os.chdir(path_dict[\"nnUNet_results\"])\n",
        "!gdown 'https://drive.google.com/uc?export=download&id=1gCdlj-O3hgoMDDw2NM8QxvFIGGVbT9Qy'\n",
        "!nnUNetv2_install_pretrained_model_from_zip nnUNetv2_3dfullres_Model_Dataset005_Prostate.zip\n",
        "!rm 'nnUNetv2_3dfullres_Model_Dataset005_Prostate.zip'\n",
        "os.chdir(base_dir)\n",
        "\n",
        "# takes roughly 9 minutes"
      ],
      "metadata": {
        "id": "nLZh7WaJjG9O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "108b790e-d73f-471a-8ec0-b267cdbaa189"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?export=download&id=1gCdlj-O3hgoMDDw2NM8QxvFIGGVbT9Qy\n",
            "From (redirected): https://drive.google.com/uc?export=download&id=1gCdlj-O3hgoMDDw2NM8QxvFIGGVbT9Qy&confirm=t&uuid=b41d1763-65df-4370-a893-7564ae863e2f\n",
            "To: /content/drive/My Drive/Colab Notebooks/nnUNet_results/nnUNetv2_3dfullres_Model_Dataset005_Prostate.zip\n",
            "100% 1.66G/1.66G [00:18<00:00, 88.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess Prostate Dataset\n",
        "!nnUNetv2_convert_MSD_dataset -i \"${RAW_DATA_PATH}/Task05_Prostate\""
      ],
      "metadata": {
        "id": "Es2vmFYUjOJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "689328cb-4999-415e-a7bb-c4dc638f169d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nnUNetv2_convert_MSD_dataset\", line 8, in <module>\n",
            "    sys.exit(entry_point())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnunetv2/dataset_conversion/convert_MSD_dataset.py\", line 128, in entry_point\n",
            "    convert_msd_dataset(args.i, args.overwrite_id, args.np)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnunetv2/dataset_conversion/convert_MSD_dataset.py\", line 62, in convert_msd_dataset\n",
            "    assert len(existing_datasets) == 0, f\"Target dataset id {target_id} is already taken, please consider changing \" \\\n",
            "AssertionError: Target dataset id 5 is already taken, please consider changing it using overwrite_target_id. Conflicting dataset: ['Dataset005_Prostate'] (check nnUNet_results, nnUNet_preprocessed and nnUNet_raw!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference on the Prostate Dataset\n",
        "Use 3d_fullres nnUnet (-c) trained on the test set of dataset 5 (-d 5) and save the outputs"
      ],
      "metadata": {
        "id": "CnZQUiGajQwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use fully trained nnU-Net to make predictions on data\n",
        "!nnUNetv2_predict -i \"${nnUNet_raw}/Dataset005_Prostate/imagesTs/\" -o \"${nnUNet_results}/Dataset005_Prostate/predTs/\" -d 5 -c 3d_fullres"
      ],
      "metadata": {
        "id": "lKFpTISAjQfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b09666-5f63-4e30-92dc-b7697e1f8d34"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#######################################################################\n",
            "Please cite the following paper when using nnU-Net:\n",
            "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
            "#######################################################################\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/nnunetv2/utilities/plans_handling/plans_handler.py:37: UserWarning: Detected old nnU-Net plans format. Attempting to reconstruct network architecture parameters. If this fails, rerun nnUNetv2_plan_experiment for your dataset. If you use a custom architecture, please downgrade nnU-Net to the version you implemented this or update your implementation + plans.\n",
            "  warnings.warn(\"Detected old nnU-Net plans format. Attempting to reconstruct network architecture \"\n",
            "There are 16 cases in the source folder\n",
            "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
            "There are 16 cases that I would like to predict\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\n",
            "Predicting prostate_03:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:04<00:00,  2.48s/it]\n",
            "100% 2/2 [00:01<00:00,  1.66it/s]\n",
            "100% 2/2 [00:01<00:00,  1.65it/s]\n",
            "100% 2/2 [00:01<00:00,  1.65it/s]\n",
            "100% 2/2 [00:01<00:00,  1.64it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_03\n",
            "\n",
            "Predicting prostate_05:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.65it/s]\n",
            "100% 2/2 [00:01<00:00,  1.65it/s]\n",
            "100% 2/2 [00:01<00:00,  1.65it/s]\n",
            "100% 2/2 [00:01<00:00,  1.64it/s]\n",
            "100% 2/2 [00:01<00:00,  1.63it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_05\n",
            "\n",
            "Predicting prostate_08:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.63it/s]\n",
            "100% 2/2 [00:01<00:00,  1.64it/s]\n",
            "100% 2/2 [00:01<00:00,  1.63it/s]\n",
            "100% 2/2 [00:01<00:00,  1.60it/s]\n",
            "100% 2/2 [00:01<00:00,  1.62it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_08\n",
            "\n",
            "Predicting prostate_09:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.61it/s]\n",
            "100% 2/2 [00:01<00:00,  1.63it/s]\n",
            "100% 2/2 [00:01<00:00,  1.62it/s]\n",
            "100% 2/2 [00:01<00:00,  1.61it/s]\n",
            "100% 2/2 [00:01<00:00,  1.62it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_09\n",
            "\n",
            "Predicting prostate_11:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.59it/s]\n",
            "100% 2/2 [00:01<00:00,  1.61it/s]\n",
            "100% 2/2 [00:01<00:00,  1.60it/s]\n",
            "100% 2/2 [00:01<00:00,  1.61it/s]\n",
            "100% 2/2 [00:01<00:00,  1.60it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_11\n",
            "\n",
            "Predicting prostate_12:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.58it/s]\n",
            "100% 2/2 [00:01<00:00,  1.59it/s]\n",
            "100% 2/2 [00:01<00:00,  1.60it/s]\n",
            "100% 2/2 [00:01<00:00,  1.59it/s]\n",
            "100% 2/2 [00:01<00:00,  1.60it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_12\n",
            "\n",
            "Predicting prostate_15:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.59it/s]\n",
            "100% 2/2 [00:01<00:00,  1.58it/s]\n",
            "100% 2/2 [00:01<00:00,  1.58it/s]\n",
            "100% 2/2 [00:01<00:00,  1.58it/s]\n",
            "100% 2/2 [00:01<00:00,  1.57it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_15\n",
            "\n",
            "Predicting prostate_19:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.57it/s]\n",
            "100% 2/2 [00:01<00:00,  1.57it/s]\n",
            "100% 2/2 [00:01<00:00,  1.57it/s]\n",
            "100% 2/2 [00:01<00:00,  1.56it/s]\n",
            "100% 2/2 [00:01<00:00,  1.56it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_19\n",
            "\n",
            "Predicting prostate_22:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.57it/s]\n",
            "100% 2/2 [00:01<00:00,  1.57it/s]\n",
            "100% 2/2 [00:01<00:00,  1.57it/s]\n",
            "100% 2/2 [00:01<00:00,  1.56it/s]\n",
            "100% 2/2 [00:01<00:00,  1.56it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_22\n",
            "\n",
            "Predicting prostate_23:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.55it/s]\n",
            "100% 2/2 [00:01<00:00,  1.52it/s]\n",
            "100% 2/2 [00:01<00:00,  1.55it/s]\n",
            "100% 2/2 [00:01<00:00,  1.54it/s]\n",
            "100% 2/2 [00:01<00:00,  1.54it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_23\n",
            "\n",
            "Predicting prostate_26:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.54it/s]\n",
            "100% 2/2 [00:01<00:00,  1.54it/s]\n",
            "100% 2/2 [00:01<00:00,  1.54it/s]\n",
            "100% 2/2 [00:01<00:00,  1.53it/s]\n",
            "100% 2/2 [00:01<00:00,  1.53it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_26\n",
            "\n",
            "Predicting prostate_27:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.52it/s]\n",
            "100% 2/2 [00:01<00:00,  1.52it/s]\n",
            "100% 2/2 [00:01<00:00,  1.52it/s]\n",
            "100% 2/2 [00:01<00:00,  1.51it/s]\n",
            "100% 2/2 [00:01<00:00,  1.52it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_27\n",
            "\n",
            "Predicting prostate_30:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.52it/s]\n",
            "100% 2/2 [00:01<00:00,  1.52it/s]\n",
            "100% 2/2 [00:01<00:00,  1.52it/s]\n",
            "100% 2/2 [00:01<00:00,  1.53it/s]\n",
            "100% 2/2 [00:01<00:00,  1.52it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_30\n",
            "\n",
            "Predicting prostate_33:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.53it/s]\n",
            "100% 2/2 [00:01<00:00,  1.53it/s]\n",
            "100% 2/2 [00:01<00:00,  1.53it/s]\n",
            "100% 2/2 [00:01<00:00,  1.52it/s]\n",
            "100% 2/2 [00:01<00:00,  1.53it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_33\n",
            "\n",
            "Predicting prostate_36:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.54it/s]\n",
            "100% 2/2 [00:01<00:00,  1.54it/s]\n",
            "100% 2/2 [00:01<00:00,  1.54it/s]\n",
            "100% 2/2 [00:01<00:00,  1.55it/s]\n",
            "100% 2/2 [00:01<00:00,  1.55it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_36\n",
            "\n",
            "Predicting prostate_45:\n",
            "perform_everything_on_device: True\n",
            "100% 2/2 [00:01<00:00,  1.56it/s]\n",
            "100% 2/2 [00:01<00:00,  1.55it/s]\n",
            "100% 2/2 [00:01<00:00,  1.55it/s]\n",
            "100% 2/2 [00:01<00:00,  1.55it/s]\n",
            "100% 2/2 [00:01<00:00,  1.54it/s]\n",
            "sending off prediction to background worker for resampling and export\n",
            "done with prostate_45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of Data and Segmentations\n",
        "Download files from Google Drive:\n",
        "\n",
        "Images from: ```${nnUNet_raw}/Dataset005_Prostate/imagesTs/```\n",
        "\n",
        "Segmentations from: ```${RESULTS_FOLDER}/Dataset005_Prostate/predTs/\"```\n",
        "\n",
        "\n",
        "After downloading these files you can visualize them with any volumetric visualization program.\n",
        "For this we would advise to use [MITK](https://www.mitk.org/wiki/The_Medical_Imaging_Interaction_Toolkit_(MITK)) which already has some great [tutorials](https://www.mitk.org/wiki/Tutorials).\n",
        "\n",
        "\n",
        "Note:\n",
        "- If you have not already downloaded it, here is the [MITK Download Link](https://www.mitk.org/wiki/Downloads)"
      ],
      "metadata": {
        "id": "IaB8Aw7SjXHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. How to train and use nnU-Net on a new Dataset?\n",
        "\n",
        "Goal of the next Steps:\n",
        "- How can you Implement nnU-Net for a new Dataset?\n",
        "  - Example using the GM Spinal Cord Segmentation Challenge Dataset\n",
        "    - [Data & Task Explanation](https://www.sciencedirect.com/science/article/pii/S1053811917302185)\n",
        "    - [Data Download Link](http://cmictig.cs.ucl.ac.uk/niftyweb/challenge/)\n",
        "- How to store the Data for use with nnU-Net?\n",
        "  - General Information can be found [here](https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/dataset_conversion.md)\n",
        "- How to create the Data Fingerprint?\n",
        "- How to create the Pipeline Fingerprint based on Rules?\n",
        "\n",
        "This part of the Tutorial based on:\n",
        "\n",
        "The [GoogleColab Notebook](https://github.com/prateekgupta891/nnUNet/blob/master/nnunetmec2020.ipynb) and the [Medium Article about nnU-Net](https://medium.com/miccai-educational-initiative/nnu-net-the-no-new-unet-for-automatic-segmentation-8d655f3f6d2a)\n",
        "written in the context of the MICCAI Educational Initiative by:\n",
        "- Prateek Gupta, Indian Institute of Information Technology, Pune (prateekgupta16@alumni.iiitp.ac.in)\n",
        "- Kumar T. Rajamani, Institute of Medical Informatics, University of Lübeck, Germany (kumar.rajamani@uni-luebeck.de)\n",
        "- Mattias P. Heinrich, Institute of Medical Informatics, University of Lübeck, Germany (heinrich@imi.uni-luebeck.de)\n",
        "\n",
        "Many Thanks to them!"
      ],
      "metadata": {
        "id": "-dAUpH3jjizL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Folderstructure for the new dataset!\n",
        "dataset_name = 'Dataset501_SCGM' #change here for different dataset name\n",
        "nnunet_raw_data = os.path.join(os.getenv(\"nnUNet_raw\"))\n",
        "task_folder_name = os.path.join(nnunet_raw_data,dataset_name)\n",
        "train_image_dir = os.path.join(task_folder_name,'imagesTr')\n",
        "train_label_dir = os.path.join(task_folder_name,'labelsTr')\n",
        "test_dir = os.path.join(task_folder_name,'imagesTs')\n",
        "\n",
        "# Create Folder Structure for the SCGM dataset on the system\n",
        "make_if_dont_exist(task_folder_name)\n",
        "make_if_dont_exist(train_image_dir)\n",
        "make_if_dont_exist(train_label_dir)\n",
        "make_if_dont_exist(test_dir)\n",
        "\n",
        "training_data_name=\"training-data-gm-sc-challenge-ismrm16-v20160302b\"\n",
        "test_data_name=\"test-data-gm-sc-challenge-ismrm16-v20160401\""
      ],
      "metadata": {
        "id": "Yic_BUF3jds5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Download\n",
        "Again, now the data has to be put into the proper place in the folder structure.\n",
        "\n",
        "In case you are not attending an in person workshop please skip the next cell"
      ],
      "metadata": {
        "id": "Usr0NZs5jq_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only for in poerson workshops\n",
        "\n",
        "os.chdir(task_folder_name)\n",
        "# download training data\n",
        "!gdown 'https://drive.google.com/uc?export=download&id=1gCdlj-O3hgoMDDw2NM8QxvFIGGVbT9Qy'\n",
        "!ls\n",
        "os.chdir(base_dir)"
      ],
      "metadata": {
        "id": "6sTebi1YjoM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# continue from here again.\n",
        "if os.path.isfile(os.path.join(task_folder_name, training_data_name+'.zip')) is False:\n",
        "  print(\"Please download the dataset zipfiles and place them into the following directory: \\n {}\".format(task_folder_name))\n",
        "else:\n",
        "  print(f'Training file for exists SCGM Challenge exists')"
      ],
      "metadata": {
        "id": "vuinAqDNjvUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify that files are in the correct place!\n",
        "os.chdir(task_folder_name)\n",
        "!ls\n",
        "if os.path.isfile(training_data_name+'.zip'):\n",
        "    print(f'Training file for exists SCGM Challenge exists')\n",
        "else:\n",
        "    print('Training file for SCGM Challenge is not present in the directory')\n",
        "    print(\"Please check whether {}.zip is in Folder {}\".format(training_data_name, task_folder_name))\n",
        "\n",
        "os.chdir(base_dir)\n",
        "print(\"We are currently in working directory {}\".format(os.getcwd()))"
      ],
      "metadata": {
        "id": "naviG5kYjxZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unzipping in nnUNet_raw folder the training data\n",
        "os.chdir(task_folder_name)\n",
        "!unzip -o training-data-gm-sc-challenge-ismrm16-v20160302b.zip\n",
        "!rm training-data-gm-sc-challenge-ismrm16-v20160302b.zip\n",
        "os.chdir(base_dir)"
      ],
      "metadata": {
        "id": "uIjYFvyEj0KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "We have 4 annotation of the same image, by different experts in the SCGM Challenge. ( Image , Ann1 ) and ( Image , Ann2 ) can be considered as a different image and label pairs. Hence, 4 copies of the training .nii.gz file is created with its mapping to the respective label name.\n",
        "\n",
        "For this the data is renamed and relocated"
      ],
      "metadata": {
        "id": "iXGRpjmtj3jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for copying, savind and renaming\n",
        "def copy_and_rename(old_location,old_file_name,new_location,new_filename,delete_original = False):\n",
        "\n",
        "    shutil.copy(os.path.join(old_location,old_file_name),new_location)\n",
        "    os.rename(os.path.join(new_location,old_file_name),os.path.join(new_location,new_filename))\n",
        "    if delete_original:\n",
        "        os.remove(os.path.join(old_location,old_file_name))\n",
        "\n",
        "# putting training images into folder\n",
        "\n",
        "mask_count = 4 # change if more mask is available\n",
        "base_data_folder_name = os.path.join(task_folder_name, \"training-data-gm-sc-challenge-ismrm16-v20160302b\")\n",
        "\n",
        "for file in os.listdir(base_data_folder_name):\n",
        "    # print(file)\n",
        "    if file.endswith('.nii.gz'):\n",
        "        if file.find('mask')!=-1:\n",
        "            # putting mask\n",
        "            shutil.move(os.path.join(base_data_folder_name,file),train_label_dir)\n",
        "        else:\n",
        "            # making 4 copies\n",
        "            for mask in range(1,mask_count+1):\n",
        "                new_filename = file[:file.find('-image')] + '-mask-r' + str(mask) + '.nii.gz'\n",
        "                if mask==mask_count:\n",
        "                    copy_and_rename(base_data_folder_name,file,train_image_dir,new_filename,delete_original = True)\n",
        "                else:\n",
        "                    copy_and_rename(base_data_folder_name,file,train_image_dir,new_filename)\n",
        "    # removing all other files installed due to the unzip\n",
        "    elif file.endswith('.txt'):\n",
        "        os.remove(os.path.join(base_data_folder_name,file))"
      ],
      "metadata": {
        "id": "Cgiv26wqj05I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verification of Data\n",
        "\n",
        "Before going any further, verify that the data is present and labels and data matches."
      ],
      "metadata": {
        "id": "SKUBron4j8a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_files = os.listdir(train_image_dir)\n",
        "label_files = os.listdir(train_label_dir)\n",
        "print(\"train image files:\",len(train_files))\n",
        "print(\"train label files:\",len(label_files))\n",
        "print(\"Matches:\",len(set(train_files).intersection(set(label_files))))\n",
        "\n",
        "assert len(set(train_files).intersection(set(label_files))) == 160 #should be equal to 160 for SCGM Challenge"
      ],
      "metadata": {
        "id": "F6tpw_YUkCMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#renaming to add the modality for SCGM there is only one modality\n",
        "#images should be added with 0000\n",
        "#can be skipped if modality is already mentioned\n",
        "#re-write for multiple modalities\n",
        "\n",
        "def check_modality(filename):\n",
        "    \"\"\"\n",
        "    check for the existence of modality\n",
        "    return False if modality is not found else True\n",
        "    \"\"\"\n",
        "    end = filename.find('.nii.gz')\n",
        "    modality = filename[end-4:end]\n",
        "    for mod in modality:\n",
        "        if not(ord(mod)>=48 and ord(mod)<=57): #if not in 0 to 9 digits\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def rename_for_single_modality(directory):\n",
        "\n",
        "    for file in os.listdir(directory):\n",
        "\n",
        "        if check_modality(file)==False:\n",
        "            new_name = file[:file.find('.nii.gz')]+\"_0000.nii.gz\"\n",
        "            os.rename(os.path.join(directory,file),os.path.join(directory,new_name))\n",
        "            print(f\"Renamed to {new_name}\")\n",
        "        else:\n",
        "            print(f\"Modality present: {file}\")\n",
        "\n",
        "rename_for_single_modality(train_image_dir)\n",
        "\n",
        "# again skip test due to non available data\n",
        "# rename_for_single_modality(test_dir)"
      ],
      "metadata": {
        "id": "OZmxhFnXkFEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creation of the Dataset501 for the SCGMC Dataset\n",
        "The Dataset gives nnU-Net Information for the Planning and Preprocessing Step.\n",
        "\n",
        "Based on the parameters of the dataset, the \"optimal strategy\" is selected (data fingerprint & pipeline fingerprint)\n",
        "\n",
        "This is done by creating the ```dataset.json``` file."
      ],
      "metadata": {
        "id": "Msudn8uakLZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overwrite_json_file = True #make it True if you want to overwrite the dataset.json file in Dataset_folder\n",
        "json_file_exist = False\n",
        "\n",
        "if os.path.exists(os.path.join(task_folder_name,'dataset.json')):\n",
        "    print('dataset.json already exist!')\n",
        "    json_file_exist = True\n",
        "\n",
        "if json_file_exist==False or overwrite_json_file:\n",
        "\n",
        "    json_dict = OrderedDict()\n",
        "    json_dict['name'] = dataset_name\n",
        "    json_dict['description'] = \"Spinal Cord Grey Matter Segmenation Challenge\"\n",
        "    json_dict['tensorImageSize'] = \"3D\"\n",
        "    json_dict['reference'] = \"see challenge website\"\n",
        "    json_dict['licence'] = \"see challenge website\"\n",
        "    json_dict['release'] = \"0.0\"\n",
        "\n",
        "    #you may mention more than one modality\n",
        "    json_dict['channel_names'] = {\n",
        "        \"0\": \"MRI\"\n",
        "    }\n",
        "\n",
        "    # set expected file ending\n",
        "    json_dict[\"file_ending\"] = \".nii.gz\"\n",
        "\n",
        "    #label names should be mentioned for all the labels in the dataset\n",
        "    json_dict['labels'] = {\n",
        "        \"background\": 0,\n",
        "        \"grey matter\": 1,\n",
        "        \"white matter\": 2\n",
        "    }\n",
        "\n",
        "    train_ids = os.listdir(train_label_dir)\n",
        "    test_ids = os.listdir(test_dir)\n",
        "    json_dict['numTraining'] = len(train_ids)\n",
        "    json_dict['numTest'] = len(test_ids)\n",
        "\n",
        "\n",
        "    with open(os.path.join(task_folder_name,\"dataset.json\"), 'w') as f:\n",
        "        json.dump(json_dict, f, indent=4, sort_keys=True)\n",
        "\n",
        "    if os.path.exists(os.path.join(task_folder_name,'dataset.json')):\n",
        "        if json_file_exist==False:\n",
        "            print('dataset.json created!')\n",
        "        else:\n",
        "            print('dataset.json overwritten!')"
      ],
      "metadata": {
        "id": "_q_pA9PWkIlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preprocessing\n",
        "Also to ensure that nnU-Net can be trained on the dataset a integrity check is performed."
      ],
      "metadata": {
        "id": "qzrlAQeHkRcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verify that the dataset will work & create plans for configuration 3d_fullres - this may take about 10 minutes\n",
        "!nnUNetv2_plan_and_preprocess -d 501 -np 4 -c 3d_fullres --verify_dataset_integrity"
      ],
      "metadata": {
        "id": "y37A0tlgkQ2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Visualization\n",
        "At this stage it is useful to examine the training and testing data.\n",
        "\n",
        "Here is an example for how to do this in python, however we would encourage you to visualize the data with MITK-Workbench, since it allows much more interaction with the data and therefore better understanding."
      ],
      "metadata": {
        "id": "LYGr8S4wkWvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Data (with Labels)"
      ],
      "metadata": {
        "id": "6jMl9CsfkcW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing some of the training images and labels\n",
        "# (re-run to see random pick-ups)\n",
        "# only maximum of first 5 slices are plotted\n",
        "train_img_name = os.listdir(train_image_dir)[np.random.randint(0,160)]\n",
        "train_img = np.array(nib.load(os.path.join(train_image_dir,train_img_name)).dataobj)[:,:,:5]\n",
        "train_label_name = train_img_name[:train_img_name.find('_0000.nii.gz')]+'.nii.gz'\n",
        "train_label = np.array(nib.load(os.path.join(train_label_dir,train_label_name)).dataobj)[:,:,:5]\n",
        "\n",
        "print(train_img.shape,train_label.shape)\n",
        "\n",
        "max_rows = 2\n",
        "max_cols = train_img.shape[2]\n",
        "\n",
        "fig, axes = plt.subplots(nrows=max_rows, ncols=max_cols, figsize=(20,8))\n",
        "for idx in range(max_cols):\n",
        "    axes[0, idx].axis(\"off\")\n",
        "    axes[0, idx].set_title('Train Image'+str(idx+1))\n",
        "    axes[0 ,idx].imshow(train_img[:,:,idx], cmap=\"gray\")\n",
        "for idx in range(max_cols):\n",
        "    axes[1, idx].axis(\"off\")\n",
        "    axes[1, idx].set_title('Train Label'+str(idx+1))\n",
        "    axes[1, idx].imshow(train_label[:,:,idx])\n",
        "\n",
        "plt.subplots_adjust(wspace=.1, hspace=.1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xn6nMitlkdlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: In the label image, yellow color represents white Matter and green-ish color represents grey matter."
      ],
      "metadata": {
        "id": "OCh3UwJ3khxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sighting of Data and Lables with MITK-Workbench\n",
        "At this stage it is also useful to analyze the imaging data with MITK-Workbench.\n",
        "To do this, download the training data to your own local machine and visualize them via drag and drop into MITK-Workbench\n",
        "\n",
        "This is already explained in part 6.3.4"
      ],
      "metadata": {
        "id": "w1ELxQ0fknYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training nnU-Net\n",
        "\n",
        "nnU-Net stores a checkpoint every 50 epochs. If you need to continue a previous training, just add a -c to the training command.\n",
        "\n",
        "Generic Training Commands:\n",
        "\n",
        "```nnUNetv2_train Dataset_NAME_OR_ID CONFIGURATION FOLD -tr TRAINER_CLASS_NAME (additional options)```\n",
        "\n",
        "For 2D:  ```nnUNetv2_train DATASET_NAME_OR_ID 2d FOLD```\n",
        "\n",
        "For 3D Full resolution: ```nnUNetv2_train DATASET_NAME_OR_ID 3d_fullres FOLD```\n",
        "\n",
        "For Cascaded 3D:\n",
        "\n",
        "First Run lowres: ```nnUNetv2_train DATASET_NAME_OR_ID 3d_lowres FOLD```\n",
        "\n",
        "Then Run fullres: ```nnUNetv2_train DATASET_NAME_OR_ID 3d_cascade_fullres FOLD```"
      ],
      "metadata": {
        "id": "1sBxpUL-ksy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train nnU-Net with 3d_fullres model on the SCGM Task with Fold 0\n",
        "# Again Training may take a while therefore it is advised to interrupt the training after some time (e.g. after 1 epoch)\n",
        "# 1 Epoch takes approximately: over 30 minutes on GoogleColab -- you need GoogleColab Pro to run this completely\n",
        "!nnUNetv2_train 501 3d_fullres 0 -tr nnUNetTrainer_1epoch"
      ],
      "metadata": {
        "id": "FMSqnlv2kkgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Code\n",
        "\n",
        "nnUNet_find_best_configuration will print inference commands you need to use. The easiest way to run inference is to simply use these commands.\n",
        "\n",
        "For each of the desired configurations, run:\n",
        "\n",
        "```nnUNet_predict -i INPUT_FOLDER -o OUTPUT_FOLDER -d DATASET_NAME_OR_ID -m CONFIGURATION --save_npz```\n",
        "\n",
        "Only specify ```--save_npz``` if you intend to use ensembling. ```--save_npz``` will make the command save the softmax probabilities alongside of the predicted segmentation masks requiring a lot of disk space.\n",
        "\n",
        "Note: Please select a separate OUTPUT_FOLDER for each configuration!"
      ],
      "metadata": {
        "id": "Kr_E_tRLkztE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional\n",
        "!nnUNetv2_find_best_configuration -d 501"
      ],
      "metadata": {
        "id": "n818mIbhk18x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_dir = os.path.join(os.getenv(\"RESULTS_FOLDER\"),'nnUNet_Prediction_Results',dataset_name)\n",
        "make_if_dont_exist(result_dir)\n",
        "\n",
        "team_name = 'awesome_nnU-Net_team' #make sure to change for your own team name"
      ],
      "metadata": {
        "id": "5c1XeNqTk6-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**:\n",
        "\n",
        "If you interrupted the training go the given fold inside of the RESULTS_FOLDER for the task and:\n",
        "1. rename **model_best.model.pkl** to **model_final_checkpoint.model.pkl**\n",
        "2. rename **model_best.model** to **model_final_checkpoint.model**"
      ],
      "metadata": {
        "id": "kQYHWAUJlADB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#location where you want save your results, will be created if dont exist\n",
        "!nnUNetv2_predict -i '${nnUNet_raw}/Dataset501_SCGM/imagesTs' -o '${RESULTS_FOLDER}/Dataset501/predTs' -d 501 -tr nnUNetTrainer_1epoch -c 3d_fullres"
      ],
      "metadata": {
        "id": "sY-qakIYlCQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you wish to run ensembling, you can ensemble the predictions from several configurations with the following command:\n",
        "\n",
        "```nnUNet_ensemble -f FOLDER1 FOLDER2 ... -o OUTPUT_FOLDER -pp POSTPROCESSING_FILE```"
      ],
      "metadata": {
        "id": "sqjhzkDYlEc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of Predictions\n",
        "Similar to the dataset visualization we would encourage you to do this with MITIK-Workbench.\n",
        "\n",
        "Here is a quick visualization with python."
      ],
      "metadata": {
        "id": "lkK22gGdlHIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing the predicted results\n",
        "# (re-run to see random pick-ups)\n",
        "# only maximum of first 5 slices are plotted\n",
        "\n",
        "test_img_name = os.listdir(test_dir)[np.random.randint(0,40)]\n",
        "test_img = np.array(nib.load(os.path.join(test_dir,test_img_name)).dataobj)[:,:,:5]\n",
        "predicted_img_name = test_img_name[:test_img_name.find('_0000.nii.gz')]+'.nii.gz'\n",
        "predicted_label = np.array(nib.load(os.path.join(result_dir,predicted_img_name)).dataobj)[:,:,:5]\n",
        "print('Test Image Shape: ',test_img.shape)\n",
        "print(\"Predicted Image Shape:\",predicted_label.shape)\n",
        "\n",
        "max_rows = 2\n",
        "max_cols = test_img.shape[2]\n",
        "\n",
        "fig, axes = plt.subplots(nrows=max_rows, ncols=max_cols, figsize=(20,8))\n",
        "for idx in range(max_cols):\n",
        "    axes[0, idx].axis(\"off\")\n",
        "    axes[0, idx].set_title('Test Image'+str(idx+1))\n",
        "    axes[0 ,idx].imshow(test_img[:,:,idx], cmap=\"gray\")\n",
        "for idx in range(max_cols):\n",
        "    axes[1, idx].axis(\"off\")\n",
        "    axes[1, idx].set_title('Predicted Label'+str(idx+1))\n",
        "    axes[1, idx].imshow(predicted_label[:,:,idx])\n",
        "\n",
        "plt.subplots_adjust(wspace=.1, hspace=.1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b1S7PlIClD7z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}